{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9521ee252c8c>:38: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(r'D:\\Downloads\\Desktop\\chromedriver',chrome_options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data Extraction': ['Disparate Data Collection', 'Email Address Extraction', 'IP Address Extraction', 'Image Extraction', 'Phone Number Extraction', 'Pricing Extraction', 'Web Data Extraction']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9521ee252c8c>:206: DeprecationWarning: use options instead of chrome_options\n",
      "  another_driver = webdriver.Chrome(r'D:\\Downloads\\Desktop\\chromedriver',chrome_options=another_options)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Dabu bahya\\Downloads\\Desktop\\Review-Analysis\\new_product_list.csv')\n",
    "\n",
    "\n",
    "new_df = df.query('`Category`.str.startswith(\"data mining\")')\n",
    "\n",
    "\n",
    "\n",
    "# We start scraping each product site with time delay of 10s.\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(3,4):\n",
    "    url =  \"https://\" + new_df.iloc[i]['URL to Capterra Page']\n",
    "    p_name = new_df.iloc[i]['Product Name']\n",
    "    p_category = new_df.iloc[i]['Category']\n",
    "    #     print(p_name)  to be removed\n",
    "    #   print(p_category) to be removed\n",
    "\n",
    "    if re.match(r\"#reviews\", url):\n",
    "        url = re.sub(r\"#reviews\",'', url)\n",
    "    else:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(r'D:\\Downloads\\Desktop\\chromedriver',chrome_options=options)  \n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\" )\n",
    "    soup_obj = soup.find('div',{'class':'ProductSummary__SummaryPanel-mcm4i7-6 hCwGXf'})\n",
    "\n",
    "    product = soup_obj.contents[0]\n",
    "    product_info = product.select('div > div')[0].get_text(strip=True) #this is info about the product \n",
    "    #   print(product_info) to be removed\n",
    "    #for checking that if \"featured in\" exists on the site \n",
    "    x = soup_obj.contents[1].h2.get_text()\n",
    "\n",
    "    if re.match(r\"Featured In\", x):\n",
    "\n",
    "        c = soup_obj.contents[2]\n",
    "        y = c.h2.get_text(strip=True)\n",
    "        if re.match(r\"Best For\",y):\n",
    "            d = c.select('div > em')[0].get_text(strip=True) # best for \n",
    "            var = 3\n",
    "\n",
    "        else:\n",
    "            d = \"Not Available\"\n",
    "            var = 2\n",
    "\n",
    "\n",
    "        try :\n",
    "            e = soup_obj.contents[var]\n",
    "            f = e.select('div > p')[0].get_text(strip=True)  # company name\n",
    "        except:\n",
    "            f = \"Not Available\"\n",
    "\n",
    "        try :\n",
    "            g = soup_obj.contents[var]\n",
    "            h = g.select('div > p')[1].get_text(strip=True)  # product website\n",
    "        except:\n",
    "            h = \"Not Available\"  \n",
    "\n",
    "\n",
    "        i = soup_obj.contents[var]\n",
    "        j = i.select('div > p')[2].get_text(strip=True)  # founding year\n",
    "        if not j:\n",
    "            j = \"Not Available\"\n",
    "        else:\n",
    "            pass  \n",
    "\n",
    "        k = soup_obj.contents[var]\n",
    "        l = k.select('div > p')[3].get_text(strip=True)  # founding place\n",
    "        if not l:\n",
    "            l = \"Not Available\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        c = soup_obj.contents[1]\n",
    "        y = c.h2.get_text(strip=True)\n",
    "        if re.match(r\"Best For\",y):\n",
    "            d = c.select('div > em')[0].get_text(strip=True)\n",
    "            var = 2\n",
    "\n",
    "        else:\n",
    "            d = \"Not Available\"\n",
    "            var = 1\n",
    "\n",
    "\n",
    "        try :\n",
    "            e = soup_obj.contents[var]\n",
    "            f = e.select('div > p')[0].get_text(strip=True)\n",
    "        except:\n",
    "            f = \"Not Available\"\n",
    "\n",
    "        try :\n",
    "            g = soup_obj.contents[var]\n",
    "            h = g.select('div > p')[1].get_text(strip=True)\n",
    "        except:\n",
    "            h = \"Not Available\"\n",
    "\n",
    "        i = soup_obj.contents[var]\n",
    "        j = i.select('div > p')[2].get_text(strip=True)\n",
    "        if not j:\n",
    "            j = \"Not Available\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        k = soup_obj.contents[var]\n",
    "        l = k.select('div > p')[3].get_text(strip=True)\n",
    "        if not l:\n",
    "            l = \"Not Available\"\n",
    "        else:\n",
    "            pass\n",
    "    info = {'About Product': product_info,'Best For':d,'Company Name': f,'Product Website': h, 'Founding Year': j,'Founding Place': l }\n",
    "\n",
    "\n",
    "    new_soup = soup.find('div',{'class':'SpecTable__Root-sc-1jq5uy4-0 jZHgUG'})\n",
    "    len_of_new_soup = len(new_soup.contents)\n",
    "    # the listed price of the product, if given.\n",
    "    try:\n",
    "        price_heading = new_soup.contents[0].h3.get_text()\n",
    "        price = new_soup.contents[0].span.get_text()\n",
    "        listed_price = {price_heading:price}\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        price_heading = new_soup.contents[0].h3.get_text()\n",
    "        price = new_soup.contents[0].a.get_text() #there's a link attached with the price \n",
    "        listed_price = {price_heading:price}\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        price_heading = new_soup.contents[0].h3.get_text()\n",
    "        price = new_soup.contents[0].select('li > div').get_text()\n",
    "        listed_price = {price_heading:price}\n",
    "    except:\n",
    "        pass\n",
    "    # for available features in Deployment, Training,& Support\n",
    "    ndic ={}\n",
    "    for i in range(1,len_of_new_soup):\n",
    "        s_obj = new_soup.contents[i]\n",
    "        heading = s_obj.h3.get_text()\n",
    "        new_obj = s_obj.contents[1]\n",
    "        nl = []\n",
    "        for i in range(len(new_obj)):\n",
    "            an_obj = new_obj.contents[i]\n",
    "            x = an_obj.select('div > div')[0]['class']\n",
    "            y= x[1]\n",
    "            if y == 'bqMKDC':\n",
    "                q = an_obj.select('div > div')[1].get_text(strip=True)\n",
    "                nl.append(q)\n",
    "            if not nl:\n",
    "                pass\n",
    "            else:\n",
    "                ndic[heading] =nl\n",
    "        \n",
    "    #  print(ndic)  to be removed\n",
    "\n",
    "\n",
    "    another_soup_obj = soup.find('div',{'class':'Accordion__Root-sc-1jo79it-0 jGWhUg'})\n",
    "    # clicking all the buttons \n",
    "    for i in range(1,len(another_soup_obj.contents)):\n",
    "        h4 = another_soup_obj.contents[i].h4.get_text()\n",
    "        cb = \"//button[normalize-space()=\" + f'\"{h4}\"' + \"]\"\n",
    "        element = driver.find_element_by_xpath(cb)\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "    # extracting the corresponding features\n",
    "    nd = {}\n",
    "    for i in range(len(another_soup_obj)):\n",
    "        field_heading = another_soup_obj.contents[i].h4.get_text()\n",
    "        n_o_j = another_soup_obj.contents[i]\n",
    "        xyz = n_o_j.find_all('div',{'class':'FeatureList__FeatureCell-sc-1kxeq27-1 ifqTCQ'})\n",
    "        nl1 = []\n",
    "        for i in xyz:\n",
    "            zas = i.select('div > div')[1]['class']\n",
    "            sac = zas[1]\n",
    "            if sac == 'bqMKDC':\n",
    "                q = i.select('div > div')[2].get_text(strip=True)\n",
    "                nl1.append(q)\n",
    "            if  not nl1:\n",
    "                    pass\n",
    "            else:\n",
    "                nd[field_heading] = nl1\n",
    "        \n",
    "    print(nd)  #to be removed\n",
    "    driver.close()\n",
    "\n",
    "    newurl = urljoin(url,'reviews')\n",
    "    another_options = webdriver.ChromeOptions()\n",
    "    another_options.headless = True\n",
    "    another_driver = webdriver.Chrome(r'D:\\Downloads\\Desktop\\chromedriver',chrome_options=another_options)\n",
    "    another_driver.get(newurl)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #another_driver.execute_script(\"arguments[0].scrollIntoView(true);\", WebDriverWait(another_driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//button[normalize-space()=\"Show more reviews\"]'))))\n",
    "            #another_driver.execute_script(\"arguments[0].click();\", WebDriverWait(another_driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[normalize-space()=\"Show more reviews\"]'))))\n",
    "            new_element = another_driver.find_element_by_xpath('//button[normalize-space()=\"Show more reviews\"]')\n",
    "            another_driver.execute_script(\"arguments[0].click();\", new_element)\n",
    "        \n",
    "        except:\n",
    "            break\n",
    "\n",
    "    review_soup = BeautifulSoup(another_driver.page_source,\"html.parser\" )\n",
    "\n",
    "    ls = ['Overall','Ease of Use','Customer Service']\n",
    "    ratings = soup.find_all('div',{'class':'StarRating__Rating-sc-9jwzgg-1 cAGyvf'})\n",
    "    la = []\n",
    "    for x,y in zip(ls,ratings):\n",
    "        la.append({x:y.get_text(strip=True)})\n",
    "    print(la)\n",
    "    start = review_soup.find('div',{'class':'gtm-review-section'})\n",
    "    #print((start.contents))\n",
    "    try:\n",
    "        \n",
    "        perm_list = []\n",
    "\n",
    "        for i in range(len(start.contents)-1):\n",
    "            empty_list = []\n",
    "        #reviewer info\n",
    "            qw = start.contents[i]\n",
    "            personal_info = qw.find('div',{'class':'ReviewerAvatarSection__ReviewerInfo-sc-1a80501-1 cSadMy'})\n",
    "            name = personal_info.contents[0].get_text(strip=True)\n",
    "            other_info = personal_info.contents[1]\n",
    "            designation = other_info.select('div > div')[0].get_text(strip=True)\n",
    "            working_domain = other_info.select('div > div')[1].get_text(strip=True) \n",
    "            usage = other_info.select('div > div')[2].get_text(strip=True)\n",
    "            reviewer_info = {name:[designation,working_domain,usage]}\n",
    "            empty_list.append(reviewer_info)\n",
    "\n",
    "        #reviewer ratings    \n",
    "            li = ['Overall','Ease of Use','Customer Service', 'Features', ' Value for Money','Likelihood to Recommend']\n",
    "            ra = qw.find_all('div',{'class':'StarRating__Rating-sc-9jwzgg-1 kCLiav'})\n",
    "            reviewer_ratings = []\n",
    "            for x,y in zip(li,ra):\n",
    "                reviewer_ratings.append({x:y.get_text(strip=True)})\n",
    "            empty_list.append(reviewer_ratings)\n",
    "\n",
    "        #date    \n",
    "            date = qw.find('div',{'class':'ReviewSource__Root-lnjke6-0 iBAGLQ'}).contents[2].get_text(strip=True)\n",
    "            empty_list.append(date)\n",
    "\n",
    "        #review\n",
    "\n",
    "            review = qw.find('div',{'class':'ReviewCard__RightFlexItem-sc-18j15p9-3 cOmWnW ReviewCard__FlexItem-sc-18j15p9-2 bTVfYT'})\n",
    "            review_heading = review.contents[0].h3.get_text(strip=True)\n",
    "            overall = review.contents[0].find_all('div',{'class':'ReviewSection__Root-sc-189472c-0 icjcMH'})\n",
    "            review_text ={}\n",
    "            review_text['review heading'] = review_heading\n",
    "            for i in overall :\n",
    "                text = i.get_text(strip=True)\n",
    "                manip = text.split(':')\n",
    "                header = manip[0]\n",
    "                review_content = manip[1]\n",
    "                review_text[header] = review_content\n",
    "\n",
    "\n",
    "            empty_list.append(review_text)\n",
    "            perm_list.append(empty_list)\n",
    "        #print(perm_list)\n",
    "        new_dic = {'Product Name': p_name,'Category': p_category, 'Product Info': info , 'Price': listed_price, 'Overall Features': ndic ,'Category Features': nd ,'Overall Ratings': la, 'Review': perm_list}\n",
    "        #print(new_dic)\n",
    "        other = pd.DataFrame([new_dic])\n",
    "        #print(other)\n",
    "        with open('another_spss.csv', 'a', encoding='utf8') as f:\n",
    "            other.to_csv(f, header=f.tell()==0)\n",
    "    except:\n",
    "        pass\n",
    "    another_driver.close()   \n",
    "    time.sleep(10)\n",
    "\n",
    "end_time=time.time()\n",
    "total_time=end_time-start_time   \n",
    "\n",
    "\n",
    "print(total_time)\n",
    "print(len(perm_list))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
